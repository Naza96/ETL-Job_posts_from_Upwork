{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a27219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries required \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate categories and subcategories list of dictionaries\n",
    "\n",
    "#File with all categories and subcategories id's used to generate the URLS for each categorie and subcategorie, available \n",
    "#in the repository\n",
    "file_path = 'job_listings_container_html.txt'  \n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "#Reading the HTML code\n",
    "subcategories_html = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "subcategories = subcategories_html.find_all('div', class_='up-group-container', attrs={'data-test': 'up-c-group', 'data-test-key': True})\n",
    "\n",
    "categories_list = []\n",
    "\n",
    "#Generating the list of lists of dictionaries for the categories and its corresponding subcategories. All with their id\n",
    "for a, subcategorie in enumerate(subcategories):\n",
    "    subcategorie_title = subcategorie.find('div', class_='up-group-header').text.strip()\n",
    "    \n",
    "    sub2_bucket = subcategorie.find_all('li', attrs={'role': 'option', 'aria-selected': 'false', 'tabindex': '0', 'class': 'up-multi-select up-menu-item'})\n",
    "    \n",
    "    subcategories_data = []\n",
    "\n",
    "    for b, sub2 in enumerate(sub2_bucket):\n",
    "        b += 1\n",
    "        ids = sub2.select_one('input[type=\"checkbox\"][tabindex=\"-1\"][id^=\"\"]')\n",
    "        id_ = ids.get('id')\n",
    "        \n",
    "        subcategory_data = {'name': sub2.text.strip(), 'id': id_}\n",
    "        subcategories_data.append(subcategory_data)\n",
    "    \n",
    "    main_category_data = {'main_category': subcategorie_title, 'subcategories': subcategories_data}\n",
    "    categories_list.append(main_category_data)\n",
    "\n",
    "#Cleaning the main categories id's\n",
    "for a, dictionarie in enumerate(categories_list):\n",
    "    categories_list[a]['subcategories'][0]['id'] = categories_list[a]['subcategories'][0]['id'][4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04035592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying all categories and subcategories dictionarie\n",
    "categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a89391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to iterate the url of each subcategorie for all existing web pages\n",
    "def jobs_scraper(cat, sub, cat_sub_url):\n",
    "    total_jobs = []\n",
    "    total_skils = []\n",
    "    i = 0\n",
    "    \n",
    "    #Run while there are available web pages\n",
    "    while True:\n",
    "        i += 1\n",
    "        cat_sub_url = cat_sub_url + f'&page={i}'\n",
    "        driver = webdriver.Chrome()\n",
    "        \n",
    "        #Pull all job details for each web page: JobID, Category, Subcategory, job title, Job Type, Payment, Duration, Hours/Week, \n",
    "        #Experience, Job description, Posted date, scraped date\n",
    "        try:\n",
    "            #Based on the web page URl wait fot the dynamicalli loaded job posts to appear\n",
    "            driver.get(cat_sub_url)\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            job_listings_container = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-test=\"job-tile-list\"]')))\n",
    "            job_listings_container_html = job_listings_container.get_attribute(\"outerHTML\")\n",
    "            parser = BeautifulSoup(job_listings_container_html, 'lxml')\n",
    "            sample_job = parser.find('section', class_=\"up-card-section up-card-list-section up-card-hover\")\n",
    "            All_jobs = parser.find_all('section', class_=\"up-card-section up-card-list-section up-card-hover\")\n",
    "            \n",
    "            jobs_p = []\n",
    "            jobs_sk = []\n",
    "            \n",
    "            #Iterate through all job posts listed in each web page to extract all details for each job\n",
    "            for jobs in All_jobs:\n",
    "                Id2 = int(jobs['data-test-key'])\n",
    "                Job_type_details = jobs.find('strong', {\"data-test\": \"job-type\"})\n",
    "                Experience = jobs.find('span', {\"data-test\": \"contractor-tier\"}).text\n",
    "                Posted = jobs.find('span', {\"data-test\": \"UpCRelativeTime\"}).text\n",
    "                scraped = datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "                Description = jobs.find('span', {\"data-test\": \"job-description-text\"}).text.strip()\n",
    "                \n",
    "                #Determine whether it's a hourly or project job\n",
    "                if Job_type_details.text == 'Fixed-price':\n",
    "                    Job_type = 'Project'\n",
    "                    Budget = jobs.find('span', {\"data-test\": \"budget\"}).text.strip()\n",
    "                    Duration = 'Fixed-price'\n",
    "                    Hours_Week = 'Fixed-price'\n",
    "                else:\n",
    "                    Job_type = 'Job'\n",
    "                    Budget = Job_type_details.text[8:]\n",
    "                    Duration = jobs.find('span', {\"data-test\": \"duration\"}).text.strip().split(', ')[0]\n",
    "                    Hours_Week = jobs.find('span', {\"data-test\": \"duration\"}).text.strip().split(', ')[1]\n",
    "                \n",
    "                jobs_att = (\n",
    "                    Id2, cat, sub, jobs.h3.text, Job_type, Budget, Duration, Hours_Week, Experience, Description, Posted,\n",
    "                    scraped)\n",
    "                \n",
    "                jobs_p.append(jobs_att)\n",
    "                skills = jobs.find_all('a', class_=\"up-skill-badge text-muted\")\n",
    "                \n",
    "                #iterate all skills for each job post\n",
    "                for skill in skills:\n",
    "                    skills_pair = (Id2, skill.text)\n",
    "                    jobs_sk.append(skills_pair)\n",
    "            \n",
    "            #Add the jobs being aded for each web page to the main list\n",
    "            total_jobs.extend(jobs_p)\n",
    "            total_skils.extend(jobs_sk)\n",
    "        \n",
    "        #When there's no more web pages available stop the iteration\n",
    "        except TimeoutException:\n",
    "            driver.save_screenshot('C:/Users/Acer/Desktop/Jupyter_para_DS/Upwork/creenshot.png')\n",
    "            print('Iteration completed')\n",
    "            break\n",
    "    \n",
    "    #Return the main lists for all jobs and its skills\n",
    "    return total_jobs, total_skils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Selecting the Data & Analytics subcategories to generate the URL's for each and iterate them through the previously \n",
    "#defined function to scrape eache subcategorie\n",
    "\n",
    "website = 'https://www.upwork.com/nx/jobs/search/?sort=recency'\n",
    "cat = categories_list[3]['main_category']\n",
    "main = categories_list[3]['subcategories'][0]['id']\n",
    "\n",
    "tj = []\n",
    "tk = []\n",
    "\n",
    "for uid in categories_list[3]['subcategories'][1:]:\n",
    "    sub_id = uid['id']\n",
    "    sub_name = uid['name']\n",
    "    sub_website = website + f'&category2_uid={main}&subcategory2_uid={sub_id}'\n",
    "    \n",
    "    print('Scraping:', '\\n', f'Categorie: {cat}.', '\\n', f'Subcategorie: {sub_name}', '\\n', \n",
    "          f'URL: {sub_website}', '\\n')\n",
    "    \n",
    "    jobs, skills = jobs_scraper(cat, sub_name, sub_website)\n",
    "    tj.extend(jobs)\n",
    "    tk.extend(skills)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting MySQL with Python to ingest all jobs into the job table\n",
    "host = '127.0.0.1'\n",
    "user = 'root'\n",
    "password = input('Enter your MySQL password: ')\n",
    "schema = 'scraping'\n",
    "\n",
    "# Establish a connection to the database\n",
    "connection = pymysql.connect(host=host, user=user, password=password, db=schema)\n",
    "\n",
    "try:\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # SQL query to insert data into the 'jobs' table\n",
    "    sql = \"INSERT INTO jobs VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "    cursor.executemany(sql, tj)\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "finally:\n",
    "    # Close the cursor and the connection\n",
    "    cursor.close()\n",
    "    connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting MySQL with Python to ingest all skills into the skills table\n",
    "host = '127.0.0.1'\n",
    "user = 'root'\n",
    "password = input('Enter your MySQL password: ')\n",
    "schema = 'scraping'\n",
    "\n",
    "connection = pymysql.connect(host=host, user=user, password=password, db=schema)\n",
    "\n",
    "try:\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # SQL query to insert data into the 'jobs' table\n",
    "    sql = \"INSERT INTO jobs VALUES (%s, %s, %s)\"\n",
    "\n",
    "    cursor.executemany(sql, tk)\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    connection.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
